# ğŸ” Week 6: Model Evaluation and Hyperparameter Tuning (Iris Dataset)

## ğŸ¯ Task Objective

Train multiple machine learning models on the **Iris dataset**, evaluate their performance using classification metrics, and optimize them using **hyperparameter tuning techniques** like `GridSearchCV` and `RandomizedSearchCV`.

---

## ğŸ“ File

- `week6_model_evaluation_and_tuning.py`

---

## ğŸ“Š Models Used

- **Random Forest Classifier**
- **Support Vector Machine (SVM)**
- **Logistic Regression**

---

## ğŸ§ª Evaluation Metrics

Each model is evaluated using:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**

Displayed using the `classification_report` from `sklearn`.

---

## ğŸ”§ Hyperparameter Tuning

Two techniques are used for tuning:

1. **GridSearchCV** (for SVM)
   - Parameters tuned: `C`, `kernel`, `gamma`

2. **RandomizedSearchCV** (for Random Forest)
   - Parameters tuned: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`

---

## ğŸ“¦ Libraries Used

- `pandas`
- `numpy`
- `seaborn`
- `matplotlib`
- `sklearn` modules: `datasets`, `metrics`, `model_selection`, `svm`, `ensemble`, `linear_model`

---

## ğŸ’» How to Run

```bash
python Week 6: Model Evaluation and Hyperparameter Tuning (Iris Dataset).py
```
## Install dependencies with:
```
pip install pandas numpy seaborn matplotlib scikit-learn
```

## ğŸ§  Dataset
	â€¢	Iris dataset from sklearn.datasets
	â€¢	A classic dataset for classification tasks containing 150 samples of iris flowers from three species.

â¸»

## ğŸ”¢ Sample Output

Displays:
	â€¢	Performance metrics of each model before tuning
	â€¢	Best model parameters after tuning
	â€¢	Updated classification report with tuned models

â¸»
## âœ… Summary

This week emphasizes:
	â€¢	Training multiple classifiers
	â€¢	Evaluating using classification metrics
	â€¢	Improving performance via hyperparameter tuning